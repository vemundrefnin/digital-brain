<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Overview of Review - Empirical Asset Pricing via Machine Learning Using Critical reading or How To Make Sense of Publish Research"><title>Overview of Review - Empirical Asset Pricing via Machine Learning</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://brain.refnin.no/icon.png><link href=https://brain.refnin.no/styles.b3e1e36b0403ac565c9392b3e23ef3b6.min.css rel=stylesheet><link href=https://brain.refnin.no/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://brain.refnin.no/js/darkmode.db9ce2a66a0b74b3c86ceb12a83cc6d5.min.js></script>
<script src=https://brain.refnin.no/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://brain.refnin.no/js/popover.abe6a51cc7138c5dff00f151dd627ad1.min.js></script>
<script src=https://brain.refnin.no/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://brain.refnin.no/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://brain.refnin.no/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://brain.refnin.no",fetchData=Promise.all([fetch("https://brain.refnin.no/indices/linkIndex.d1ddc00ed23bfffee3b6ecf8542f8e4e.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://brain.refnin.no/indices/contentIndex.c5765fd4a962787cabe4b27b7c44d942.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://brain.refnin.no",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://brain.refnin.no",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/brain.refnin.no\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-BY8S8P23YR"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BY8S8P23YR",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://brain.refnin.no/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://brain.refnin.no>🧠 Vemund´s Digital Brain</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Overview of Review - Empirical Asset Pricing via Machine Learning</h1><p class=meta>Last updated
Oct 21, 2022
<a href=https://github.com/vemundrefnin/digital-brain/tree/hugo/content/Overview%20of%20Review%20-%20Empirical%20Asset%20Pricing%20via%20Machine%20Learning.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://brain.refnin.no/tags/engagement-paper/>Engagement paper</a></li><li><a href=https://brain.refnin.no/tags/research-review/>Research review</a></li><li><a href=https://brain.refnin.no/tags/economics/>Economics</a></li><li><a href=https://brain.refnin.no/tags/econ5100/>Econ5100</a></li></ul><a href=#overview-of-review---empirical-asset-pricing-via-machine-learning><h1 id=overview-of-review---empirical-asset-pricing-via-machine-learning><span class=hanchor arialabel=Anchor># </span>Overview of Review - Empirical Asset Pricing via Machine Learning</h1></a><p>Using
<a href=/Critical-reading-or-How-To-Make-Sense-of-Publish-Research/ rel=noopener class=internal-link data-src=/Critical-reading-or-How-To-Make-Sense-of-Publish-Research/>Critical reading or How To Make Sense of Publish Research</a></p><p>My review:
<a href=/Review-Empirical-Asset-Pricing-via-Machine-Learning/ rel=noopener class=internal-link data-src=/Review-Empirical-Asset-Pricing-via-Machine-Learning/>Review - Empirical Asset Pricing via Machine Learning</a></p><p><a href="https://www.nber.org/system/files/working_papers/w25398/w25398.pdf#page=46&zoom=100,84,573" rel=noopener>Paper link</a></p><a href=#identifying-the-authors-argument><h2 id=identifying-the-authors-argument><span class=hanchor arialabel=Anchor># </span>Identifying the Author´s Argument</h2></a><ol><li><p>What question is the author asking?
How do capable are different machine learning methods to measuring asset risk premia?
More concretely &ldquo;predicting returns in the cross section and time series&rdquo;</p></li><li><p>What answer does the author propose (i.e., what is the principle assertion of the study)?
Large economic gains to investors using machine learning forecasts.</p></li><li><p>In what ways does the study improve upon previous research?
Recently, variations of machine learning methods have been used to study the cross section of stock returns. Harvey and Liu (2016) study the multiple comparisons problem using a bootstrap procedure. Giglio and Xiu (2016) and Kelly et al. (2019) use dimension reduction methods to estimate and test factor pricing models. Moritz and Zimmermann (2016) apply tree-based models to portfolio sorting. Kozak et al. (2019) and Freyberger et al. (2019) use shrinkage and selection methods to, respectively, approximate a stochastic discount factor and a nonlinear function for expected returns.</p></li><li><p>How does this the proposed answer compare with that provided by previous research?</p></li><li><p>What are the major logic or theoretical reasons for the author´s argument?</p></li><li><p>What emperical evidence does the author provide?</p></li><li><p>What assumptions is the author making his or her reasoning?</p></li></ol><a href=#evaluating-the-authors-argument><h2 id=evaluating-the-authors-argument><span class=hanchor arialabel=Anchor># </span>Evaluating the Author´s Argument</h2></a><ol><li>Does the theoretical analysis make sense?</li><li>Are the data used adequate to the task?</li><li>Does the empirical methodology adequately?</li><li>Are the assumptions reasonable?</li><li>Is the analysis (theoretical and empirical) clearly explained?</li><li>Do the conclusions follow from the evidence presented?</li><li>On balance, is the author´s argument convincing to you?</li></ol><a href=#review-papers-content><h2 id=review-papers-content><span class=hanchor arialabel=Anchor># </span>Review Papers Content</h2></a><ul><li>What is the main research question</li><li>Method and data<ul><li>What data is used</li><li>how many years</li><li>how to clean data and collect it</li></ul></li><li>Dependent variables</li><li>Independent variables</li><li>What is done right or wrong, recommendation for improvement</li></ul><a href=#notes-from-paper><h1 id=notes-from-paper><span class=hanchor arialabel=Anchor># </span>Notes From Paper</h1></a><p>Machine learning Methods: linear regression, generalized linear models with penalization, dimension reduction via principal components regression (PCR) and partial least squares (PLS), regression trees (including boosted trees and random forests), and neural networks.</p><a href=#data><h2 id=data><span class=hanchor arialabel=Anchor># </span>Data</h2></a><p>We conduct a large scale empirical analysis, investigating nearly 30,000 individual stocks over 60 years from 1957 to 2016.
Predictor set includes 94 characteristics for each stock, interactions of each characteristic with eight aggregate time series variables, and 74 industry sector dummy variables, totaling more than 900 baseline signals.</p><p>The immediate implication is that machine learning aids in solving practical investments problems such as market timing, portfolio choice, and risk management, justifying its role in the business architecture of the fintech industry.</p><a href=#controlgroup-><h2 id=controlgroup-><span class=hanchor arialabel=Anchor># </span>Controlgroup (?)</h2></a><p>Consider as a benchmark a panel regression of individual stock returns onto three lagged stocklevel characteristics: size, book-to-market, and momentum.
Lewellen (2015) demonstrates that this model performs about as well as larger and more complex stock prediction models studied in the literature.
R2 from the benchmark model is 0.16% per month for the panel of individual stock returns.
Also controllgroup:</p><pre><code>Our work extends the empirical literature on stock return prediction, which comes in two basic strands. The first strand models differences in expected returns across stocks as a function of stocklevel characteristics, and is exemplified by Fama and French (2008) and Lewellen (2015). The typical approach in this literature runs cross-sectional regressions of future stock returns on a few lagged stock characteristics. The second strand forecasts the time series of returns and is surveyed by Koijen and Nieuwerburgh (2011) and Rapach and Zhou (2013). This literature typically conducts time series regressions of broad aggregate portfolio returns on a small number of macroeconomic predictor variables.
</code></pre><a href=#findings><h2 id=findings><span class=hanchor arialabel=Anchor># </span>Findings</h2></a><ol><li><p>Machine learning shows great promise for empirical asset pricing.
R2 into positive territory at 0.11% per month</p></li><li><p>Vast predictor sets are viable for linear prediction when either penalization or dimension reduction is used.
R2 into positive territory at 0.11% per month. Principal components regression (PCR) and partial least squares (PLS), which reduce the dimension of the predictor set to a few linear combinations of predictors, further raise the out-of-sample R2 to 0.26% and 0.27%, respectively.</p></li><li><p>Allowing for nonlinearities substantially improves predictions.
Accived with generalized linear models, regression trees, and neural networks.
monthly stock-level R2 ’s between 0.33% and 0.40%.</p></li><li><p>Shallow learning outperforms deeper learning.
neural network performance peaks at three hidden layers then declines as more layers are added. Likewise, the boosted tree and random forest algorithms tend to select trees with few “leaves” (on average less than six leaves) in our analysis.</p></li><li><p>The distance between nonlinear methods and the benchmark widens when predicting portfolio returns.
By aggregating stock-level forecasts from the benchmark three-characteristic OLS model, we find a monthly S&P 500 predictive R2 of −0.22%. The bottom-up S&P 500 forecast from the generalized linear model, in contrast, delivers an R2 of 0.71%. Trees and neural networks improve upon this further, generating monthly out-of-sample R2 ’s between 1.08% to 1.80% per month.</p></li><li><p>The economic gains from machine learning forecasts are large.
Our tests show clear statistical rejections of the OLS benchmark and other linear models in favor of nonlinear machine learning tools.</p></li><li><p>The most successful predictors are price trends, liquidity, and volatility</p></li><li><p>Better understanding our machine learning findings through simulation.
When we apply our machine learning repertoire to the simulated datasets, we find that linear and generalized linear methods dominate in the linear and uninteracted setting, yet tree-based methods and neural networks significantly outperform in the nonlinear and interactive setting.</p></li></ol><a href=#method><h2 id=method><span class=hanchor arialabel=Anchor># </span>Method:</h2></a><p>They compare thirteen models in total, including OLS with all covariates, OLS-3 (which pre-selects size, book-to-market, and momentum as the only covariates), PLS, PCR, elastic net (ENet), generalized linear model with group lasso (GLM), random forest (RF), gradient boosted regression trees (GBRT), and neural network architectures with one to five layers (NN1,…,NN5). For OLS, ENet, GLM, and GBRT, we present their robust versions using Huber loss, which perform better than the version without.</p><p><img src=https://brain.refnin.no/attachments/Pasted%20image%2020220929174854.png width=auto alt>
<img src=https://brain.refnin.no/attachments/Pasted%20image%2020220929175330.png width=auto alt>
The comparative performance across different methods is similar to the monthly results shown in Table 1, but the annual R2 oos is nearly an order of magnitude larger. Their success in forecasting annual returns likewise illustrates that
machine learning models are able to isolate risk premia that persist over business cycle frequencies and are not merely capturing short-lived inefficiencies.</p><p>Figures 4 and 5 demonstrate that models are generally in close agreement regarding the most influential stock-level predictors, which can be grouped into four categories.</p><p>The first are based on recent price trends, including five of the top seven variables in Figure 5: short-term reversal (mom1m), stock momentum (mom12m), momentum change (chmom), industry momentum (indmom), recent maximum return (maxret), and long-term reversal (mom36m).</p><p>Next are liquidity variables, including turnover and turnover volatility (turn, std turn), log market equity (mvel1), dollar volume (dolvol), Amihud illiquidity (ill), number of zero trading days (zerotrade), and bidask spread (baspread).</p><p>Risk measures constitute the third influential group, including total and idiosyncratic return volatility (retvol, idiovol), market beta (beta), and beta-squared (betasq).</p><p>The last group includes valuation ratios and fundamental signals, such as earnings-to-price (ep), sales-toprice (sp), asset growth (agr), and number of recent earnings increases (nincr). Figure 4 shows that characteristic importance magnitudes for penalized linear models and dimension reduction models are highly skewed toward momentum and reversal. Trees and neural networks are more democratic, drawing predictive information from a broader set of characteristics.</p><p><img src=https://brain.refnin.no/attachments/Pasted%20image%2020220929181507.png width=auto alt>
We also construct eight macroeconomic predictors following the variable definitions detailed in Welch and Goyal (2008), including dividend-price ratio (dp), earnings-price ratio (ep), book-tomarket ratio (bm), net equity expansion (ntis), Treasury-bill rate (tbl), term spread (tms), default spread (dfy), and stock variance (svar).3</p><p><img src=https://brain.refnin.no/attachments/Pasted%20image%2020220929182206.png width=auto alt>
All models agree that the aggregate book-to-market ratio is a critical predictor, whereas market volatility has little role in any model.</p><a href=#machine-poritfolio><h2 id=machine-poritfolio><span class=hanchor arialabel=Anchor># </span>Machine Poritfolio</h2></a><p>The best 10–1 strategy comes from NN4, which returns on average 2.3% per month (27.1% on an annualized basis). Its monthly volatility is 5.8% (20.1% annualized), amounting to an annualized out-of-sample Sharpe ratio of 1.35</p><a href=#simple-linear><h3 id=simple-linear><span class=hanchor arialabel=Anchor># </span>Simple Linear</h3></a><p>The baseline estimation of the simple linear model uses a standard least squares, or “l2”, objective function
$$L(\theta) = \frac{1}{NT}\sum^{N}<em>{i=1}\sum^{T}</em>{t=1}(r_{i,t+1}-g(z_{i,t};\theta))^2$$</p><a href=#extension---robust-objective-function><h4 id=extension---robust-objective-function><span class=hanchor arialabel=Anchor># </span>Extension - Robust Objective Function</h4></a><p>Can somtimes improve predictive performance by replacing equation (4) with a weighted least squares objective such as
$$L(\theta) = \frac{1}{NT}\sum^{N}<em>{i=1}\sum^{T}</em>{t=1}w_{i,t}(r_{i,t+1}-g(z_{i,t};\theta))^2$$</p><p>In the machine learning literature, a common choice for counteracting the deleterious effect of heavy-tailed observations is the Huber robust objective function, defined as</p><p>$$L(\theta) = \frac{1}{NT}\sum^{N}<em>{i=1}\sum^{T}</em>{t=1}H(r_{i,t+1}-g(z_{i,t};\theta), \xi)$$
where
$$
H(x;\xi) =
\Big{
\begin{matrix}
x^{2}, \text{ if } |x|\leq \xi;\\2\xi |x|-\xi^2, text{ if } |x|\geq \xi.\\\end{matrix}
$$
The Huber loss, H(·), is a hybrid of squared loss for relatively small errors and absolute loss for relatively large errors, where the combination is controlled by a tuning parameter, ξ, that can be optimized adaptively from the data.
In the empirical analysis they study the predictive benefits of robust loss functions in multiple machine learning methods.</p><a href=#penalized-linear><h3 id=penalized-linear><span class=hanchor arialabel=Anchor># </span>Penalized Linear</h3></a><p>When the number of predictors P approaches the number of observations T, the linear model becomes inefficient or even inconsistent.</p><p>Penalized methods differ by appending a penalty to the original loss function
$$L(θ; ·) =L(θ) + φ(θ; ·)$$
There are several choices for the penalty function φ(θ; ·). We focus on the popular “elastic net” penalty, which takes the form
$$\Phi(\theta; \lambda; p) = \lambda(1-p)\sum^{P}<em>{j=1}|\theta_j|+\frac{1}{2}\lambda p \sum^{P}</em>{j=1}\theta^2_{j}$$
They adaptively optimize the tuning parameters, λ and ρ, using the validation sample. Our implementation of penalized regression uses the accelerated proximal gradient algorithm and accommodates both least squares and Huber objective functions.</p><a href=#dimension-reduction-pcr-and-pl><h3 id=dimension-reduction-pcr-and-pl><span class=hanchor arialabel=Anchor># </span>Dimension Reduction: PCR and PL</h3></a><p>…</p><a href=#generalized-linear><h3 id=generalized-linear><span class=hanchor arialabel=Anchor># </span>Generalized Linear</h3></a><p>They first and most straightforward nonparametric approach that they consider is the generalized linear model. It introduces nonlinear transformations of the original predictors as new additive terms in an otherwise linear model. Generalized linear models are thus the closest nonlinear counterparts to the linear approaches.</p><p>Forecasting with the generalized linear model can be approached with the same estimation tools as in &ldquo;Simple Linear&rdquo;. In particular, our analysis uses a least squares objective function, both with and without the Huber robustness modification. Because series expansion quickly multiplies the number of model parameters, we use penalization to control degrees of freedom.</p><a href=#boosted-regression-trees-and-random-forest><h3 id=boosted-regression-trees-and-random-forest><span class=hanchor arialabel=Anchor># </span>Boosted Regression Trees and Random Forest</h3></a><p>Unlike linear models, trees are fully nonparametric and possess a logic that departs markedly from traditional regressions.
<img src=https://brain.refnin.no/attachments/Pasted%20image%2020220929171900.png width=auto alt></p><p>Used boosting and random forest.</p><a href=#neural-networks><h3 id=neural-networks><span class=hanchor arialabel=Anchor># </span>Neural Networks</h3></a><p>They are the currently preferred approach for complex machine learning problems such as computer vision, natural language processing, and automated game-playing.</p><ul><li>traditional “feed-forward” networks.
<img src=https://brain.refnin.no/attachments/Pasted%20image%2020220929173026.png width=auto alt>
There are many potential choices for the nonlinear activation function (such as sigmoid, hyperbolic, softmax, etc.). We use the same activation function at all nodes, and choose a popular functional form in recent literature known as the rectified linear unit (ReLU), defined as</li></ul><p>$$ReLU(x) =
\Big{
\begin{matrix}
0, \text{ if } |x|\leq 0\\x, \text{ otherwise}\\\end{matrix}$$</p><p>which encourages sparsity in the number of active neurons, and allows for faster derivative evaluation.</p><p>In addition to l1 penalization of the weight parameters, we simultaneously employ four other regularization techniques in our estimation: learning rate shrinkage, early stopping, batch normalization, and ensembles</p><a href=#critiques><h2 id=critiques><span class=hanchor arialabel=Anchor># </span>Critiques</h2></a><ul><li>Are they qualified?</li><li>is the model overfitted?
<img src=https://brain.refnin.no/attachments/Pasted%20image%2020220929172723.png width=auto alt></li><li>Difficult to trust because it is difficult to understand.</li><li>Different controllgroup</li></ul></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://brain.refnin.no/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Vemund E. Refnin using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2022</p><ul><li><a href=https://brain.refnin.no>Home</a></li><li><a href=https://refnin.no>Refnin.no</a></li><li><a href=https://github.com/vemundrefnin>Github</a></li></ul></footer></div></div></body></html>